---
title: "xgboost"
output: html_document
date: '2022-12-12'
---

# imputed by mean, categorical coded into dummies
```{r}
train <- read.csv("train.csv")
train_data <- train[, -1] # dropped outcome
test <- read.csv("test.csv")
test_data <- test[, -1] # dropped outcome

train_label <- train$hospital_death
test_label <- test$hospital_death
```

# need to balance/ weight the outcome variable still
```{r}
sum(train$hospital_death == 1)/nrow(train)*100 # 8.66626% died
sum(test$hospital_death == 1)/nrow(test)*100 # 8.521959% died
```

XGboost is built to manage huge datasets very efficiently!
```{r}
library(xgboost)
```


### XGboost with cross validatoin

```{r}
set.seed(123)
xgb_cv <- xgboost::xgb.cv(data = as.matrix(train_data), label = train_label,
                          nrounds = 20, eval_metric = "error", nfold = 5,
                          max_depth = 6, objective = "binary:logistic", 
                          early_stopping_rounds = 5)

xgb_fit <- xgboost::xgboost(data = as.matrix(train_data), label = train_label, 
                            nround = xgb_cv$best_iteration, nfold = 5,
                            eval_metric = "error", max_depth = 6, 
                            objective = "binary:logistic", early_stopping_rounds = 5)
```

Last iteration gave the lowest test error of 0.0703504, with a corresponding training
error of 0.0534464.

```{r}
set.seed(123)
xgb_train <- as.numeric(xgboost:::predict.xgb.Booster(xgb_fit, newdata = as.matrix(train_data)) > 0.5)
xgb_test <- as.numeric(xgboost:::predict.xgb.Booster(xgb_fit, newdata = as.matrix(test_data)) > 0.5)
 
tab <- table(train_label, xgb_train)
train_misclass <- 1-max(c(sum(diag(tab)), tab[1,2]+tab[2,1]))/sum(tab)
train_misclass

tab <- table(test_label, xgb_test)
test_misclass <- 1-max(c(sum(diag(tab)), tab[1,2]+tab[2,1]))/sum(tab)
test_misclass
```
Train mis-classification: 0.05582694
Test classification: 0.07021676

Plot!!
```{r}
# feature of importance
importance_mat <- xgboost::xgb.importance(model = xgb_fit)
xgboost::xgb.plot.importance(importance_mat, 
                             main = "Feature Importance", 
                             xlab = "Importance measure")
```

Not all the features are able to fit in the plot above, the rest are of even less
importance so we just selected this plot for the sake of more clarity and simplicity.
There are 38 features shown above.

6 diagnostics: accuracy, f1 score, recall, precision, auc, balanced accuracy

```{r}
library(caret)
# confusion matrix

conf_mat = confusionMatrix(data = as.factor(xgb_test), reference = as.factor(test$hospital_death),
                           positive="1")
conf_mat

# auc or roc for trained model
auc(test$hospital_death, xgb_test)
precision <- posPredValue(as.factor(xgb_test), as.factor(test$hospital_death), positive = "1")
precision
recall <- sensitivity(as.factor(xgb_test), as.factor(test$hospital_death), positive="1")
recall
f1 <- (2 * precision * recall) / (precision + recall)
f1
```

ROC is a probability curve and AUC represents the degree or measure of separability. 
It tells how much the model is capable of distinguishing between classes. 
Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1.

Accuracy measures how much of the data you labeled correctly.
Precision is a measure that tells you how often that something you label as positive
is actually positive. (TP/(TP+FP))
